CS130 Project 4 - Design Document
=================================

Please answer all questions in this design document.  Note that the final
feedback section is optional, and you are not required to answer it if you
don't want to.

Unanswered or incompletely answered questions, or answers that don't actually
match the code/repository, will result in deductions.

Answers don't have to be deeply detailed!  We are mainly looking for an
overview or summary description of how your project works, and your team's
experiences working on this project.

Logistics (7 pts)
-----------------

L1.  [2pts] Enumerate all teammates here.
     Patrick Donohoe
     Teresa Huang
     Carolyn Lu

L2.  [2pts] What did each teammate focus on during this project?
     Patrick Donohoe - focused on fixing acceptance tests from last week,
     helped with lark implementations for project 4
     Teresa Huang - focused on implementing project 4
     Carolyn Lu - focused on testing and performance tests, helped with fixing
     issues from last week

L3.  [3pts] Approximately how many hours did each teammate spend on the project?
     Patrick Donohoe - 15 hours
     Teresa Huang - 15 hours 
     Carolyn Lu - 15 hours

Spreadsheet Engine Design (31 pts)
----------------------------------

D1.  [3pts] Briefly describe the changes you made to the Lark parser grammar
     to support Boolean literals.
     
     We added in a BOOL lexer terminal so that it wouldn't be discarded from
     the parse tree and included it as one of the base values in the grammar. 
     In our lark parser itself, we included a bool() method whose sub-branches
     the Interpreter will visit. To make sure the other operations also now
     handled booleans (such as concatenation), we included multiple helper 
     methods to convert booleans to different types, along with different types
     to booleans.

D2.  [4pts] Briefly describe the changes you made to the Lark parser grammar
     to support conditional expressions.  How did you ensure that conditional
     operations are lower precedence than arithmetic and string concatenation
     operations?

     To support conditional expressions like comparisons, we added a COMP_OP 
     lexer terminal that included all of the comparator operators along with 
     a comp_expr rule. In order to ensure that comp_expr was the lowest
     presedence rule, we changed the grammar so that comp_expr was the only
     expression, so that is executed last, with all other rules now starting
     from the comp_expr rule. To ensure that conditional operations
     are lower precedence than arithmetic and string concatenation
     operations, we used the rule construction to explicitly encode the
     operator precedence: ?comp_expr : (comp_expr COMP_OP)? (add_expr | concat_expr)
     comp_expr is referencing both add_expr and concat_expr, therefore ensuring
     those terms get matched before comp_expr.

D3.  [6pts] Briefly describe how function invocation works in your spreadsheet
     engine.  How easy or hard would it be for you to add new functions to your
     engine?  What about a third-party developer?  How well does your code
     follow the Open/Closed Principle?

     Funciton invocation in our spreadsheet engine is very simple. We have have
     a func rule in our grammar that identifies when an input is in the format
     of a function. If an input is in the format of a function, then our parser
     extracts the function name from the input, and sends the input to our func
     method. Inside the func method, we compare the parsed function name to a
     dictionary of function names, which are matched to their callables.
     Therefore, in order to add a new funciton, all a developer would have to
     do is make a new entry in the dictionary with a function name and a
     corresponding callable that does what the developer wants for the function.
     Our implementation largely followes the open-closed principles, given that
     any new functions added would only require a new entry in the dictionary
     of functions, and the new function code would largely be contained in a
     new callable method.

D4.  [4pts] Is your implementation able to lazily evaluate the arguments to
     functions like IF(), CHOOSE() and IFERROR()?  (Recall from the Project 4
     spec that your spreadsheet engine should not report cycles in cases where
     an argument to these functions does not need to be evaluated.)  If so,
     what changes to your design were required to achieve this?  If not, what
     prevented your team from implementing this?

     In order to lazily evaluate the arguments in the three specified functions,
     we decided that for a lot of the functions to stop using the
     @visit_children_decor. This evaluates all the arguments prior to the
     function being called. Instead, we used the self.visit method on each
     of the parents childrent that we needed to evaluate. Then, the way our
     dependency graph is filled is by dfsing through the parse tree we return
     to workbook and finding all the cell references. Therefore, in each method
     where all the arguments are not evaluated, we change the parse tree to
     reflect only the arguments that have been evaluated

D5.  [4pts] Is your implementation able to evaluate the ISERROR() function
     correctly, with respect to circular-reference errors?  (Recall from the
     Project 4 spec that ISERROR() behaves differently when part of a cycle,
     vs. being outside the cycle and referencing some cell in the cycle.)
     If so, what changes to your design were required to achieve this?  If
     not, what prevented your team from implementing this?

     We used our iterative tarjans algorithm to identify sccs in the graph,
     then, we passed in a paramenter to the parser to specify if a given
     cell was a member of an scc. If a cell is part of an scc, then we 
     raise a circ_ref error, otherwise, we defer to the traditional
     operation of the function. 

D6.  [4pts] Is your implementation able to successfully identify cycles that
     are not evident from static analysis of formulas containing INDIRECT()?
     If so, what changes to your design were required, if any, to achieve this?
     If not, what prevented your team from implementing this?

     We implemented this by utilizing the way we populate our graph as detailed
     previously. Since we use the parse tree to identfy cell references in
     formulas, for indirect, we modify the parse tree to contain the evaluated
     contents of the cell, which is the cell that it references. Therefore,
     our graph properly populates based on this revised parse tree. 

D7.  [6pts] Project 4 has a number of small but important operations to
     implement.  Comparison operations include a number of comparison and type
     conversion rules.  Different functions may require specific numbers and
     types of arguments.  How did your team structure the implementation of
     these operations?  How did your approach affect the reusability and
     testability of these operations?

     For the comparisons, we chose to use a single function with a large if
     else block. We used this because all of the comparisons are very similar,
     in that they all require the same number of arguments, and the all
     have the same type conversions.
     For the functions, each function was separated out into its own callable, 
     so that each function is able identfy if the correct number and types
     of arguments were provided. Our approach for comparasions did make is less
     reusable, given that it was less abstracted, but our approach for the
     functions was very reusable. Both types of operations with different
     structures had no issues being tested.

Performance Analysis (12 pts)
-----------------------------

In this project you must measure and analyze the performance of features that
generate large bulk changes to a workbook:  loading a workbook, copying or
renaming a sheet, and moving or copying an area of cells.  Construct some
performance tests to exercise these aspects of your engine, and use a profiler
to identify where your program is spending the bulk of its time.

A1.  [4pts] Briefly enumerate the performance tests you created to exercise
     your implementation.
     We tested load workbook, copy sheet, rename sheet, and move cells. These
     operations all require recalculation of cells if needed, so the performance
     tests made sure to maximize the number of cells needed to be recalculated.
     In the case of load workbook, we created a json with 100 x 100 cells to be
     loaded. For copy and rename sheet, the cells are recalculated if there 
     are cell references involving the new name of the sheet and any cells dependent
     on those cells. So, all 100 x 100 cells were forced to have recalculation.
     For move cells, we moved all 100 x 100 cells to a completely new sheet, 
     also stressing the performance.

A2.  [2pts] What profiler did you choose to run your performance tests with?
     Why?  Give an example of how to invoke one of your tests with the profiler.
     Like the last performance test in project 2, we chose to use cProfile
     mainly because of the output that it gave in comparison to the other two
     sampling profilers. Although the sampling profilers take less time to run,
     we felt that the information provided by cProfile was easier to access and
     understand. To invoke one of our tests with the profiler, you simply need
     to run "python3 -m unittest tests.performance.TestPerformance.{specific_test}."
     Each test already has a profiler included inside, so there is no need to
     invoke it again through the command line. To run all of the performance 
     tests at once, you simply need to run "make performance2". To start the
     profiler inside a test itself, you first need to create a
     cProfile.Profile(), enable it, run any code you want to analyze, and then
     disable it.

A3.  [6pts] What are ~3 of the most significant hot-spots you identified in your
     performance testing?  Did you expect these hot-spots, or were they
     surprising to you?
     Copy sheet took significantly more time than any other test due to its
     need for deepcopying, which in total contributed to around 25% of the
     cumulative time. Unsurprisingly, setting cell contents also was a large 
     hot-spot across all of the tests, due to all of the re-setting and
     re-calculating of cells needed. For rename sheet, update_workbook is a 
     significant hot-spot as well, due to the need to update all of a cell's
     dependent cells after changing its contents. parse_contents and lark are
     also taking a considerable amount of time. These operations were always
     known to take the longest time, due to their heavy involvement with time 
     consuming algorithms like graph traversal.

Section F:  CS130 Project 3 Feedback [OPTIONAL]
-----------------------------------------------

These questions are OPTIONAL, and you do not need to answer them.  Your grade
will not be affected by answering or not answering them.  Also, your grade will
not be affected by negative feedback - we want to know what went poorly so that
we can improve future versions of the course.

F1.  What parts of the assignment did you find highly enjoyable?  Conversely,
     what parts of the assignment did you find unenjoyable?


F2.  What parts of the assignment helped you learn more about software
     engineering best-practices, or other useful development skills?
     What parts were not helpful in learning these skills?


F3.  Were there any parts of the assignment that seemed _unnecessarily_ tedious?
     (Some parts of software development are always tedious, of course.)


F4.  Do you have any feedback and/or constructive criticism about how this
     project can be made better in future iterations of CS130?
